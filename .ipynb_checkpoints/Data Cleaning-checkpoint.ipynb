{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PNEUMONIA DETECTION USING CHEST X-RAY IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Pneumonia is a respiratory disease that causes inflammation in one or both lungs, resulting in symptoms such as cough, fever, and difficulty breathing. Early detection of pneumonia is essential for effective treatment and improved patient outcomes. It accounts for more than 15% of deaths in children under the age of five years. Therefore, early diagnosis and management can play a pivotal role in preventing the disease from becoming fatal.\n",
    "In acute respiratory diseases, human lungs are made up of small sacs called alveoli, which are normally in the air in healthy individuals. However, in pneumonia, these alveoli get filled with fluid or \"pus.\" One of the major steps in phenomena detection and treatment is obtaining the chest X-ray (CXR). Physicians use this X-ray image to diagnose or monitor treatment for pneumonia conditions. This type of chest X-ray is also used in the diagnosis of diseases like emphysema, lung cancer, line and tube placement, and tuberculosis.\n",
    "Moreover, there is significant variability in the way chest X-ray images are acquired and processed, which can impact the quality and consistency of the images. This variability can make it challenging to develop robust algorithms that can accurately identify pneumonia in all types of images. Hence, there is a need to develop robust, data-driven algorithms that are trained on large, high-quality datasets and validated using a range of imaging techniques and expert radiological analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Objectives\n",
    "1.\tResearch, design, and implement advanced algorithms for the accurate detection of pneumonia in chest X-ray images.\n",
    "2.\tConstruct a robust binary classifier capable of distinguishing between normal and pneumonia cases in chest X-ray images.\n",
    "3.\tIntegrate the developed pneumonia detection algorithms into an automated diagnostic tool for chest X-ray images.\n",
    "4.\tImprove the efficiency and precision of pneumonia diagnosis by deploying the automated diagnostic tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Objective\n",
    "The goal of this project is to develop an automated system for detecting and classifying pneumonia in medical images. \n",
    "Design and implement a robust deep learning algorithm, for detecting and classifying pneumonia in chest X-ray images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset, sourced from Kaggle and accessible at https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data , is organized into three main folders: train, test, and val. Each folder includes subfolders representing two distinct image categories: Pneumonia and Normal. The dataset comprises a total of 5,856 chest X-ray images in JPEG format. These images, employing the anterior-posterior technique, originate from retrospective cohorts of pediatric patients aged one to five years at the Guangzhou Women and Childrenâ€™s Medical Center in Guangzhou, China. The inclusion of these chest X-ray images in the dataset was part of routine clinical care for pediatric patients.\n",
    "\n",
    "Ensuring dataset quality, an initial screening process eliminated low-quality or unreadable scans to minimize errors. Two expert physicians then meticulously graded the diagnoses associated with the images, deeming them suitable for training the AI system only after this rigorous evaluation. To further mitigate potential grading errors, an additional layer of scrutiny was applied to the evaluation set. This involved examination by a third expert, providing an extra level of assurance to the accuracy of the diagnoses. This comprehensive approach to quality control and grading establishes a robust foundation for the analysis of chest X-ray images, enhancing the reliability of the AI system trained on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "import opendatasets as od\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import random\n",
    "from pathlib import Path #to be able to use functions using path\n",
    "from collections import Counter\n",
    "\n",
    "# Data science tools\n",
    "import pandas as pd # data processing\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# Tensorflow for GPU\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import Session, ConfigProto, set_random_seed\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Keras library for Modeling\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "#import tensorflow.contrib.keras as keras\n",
    "from keras import backend as K\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# Resize images\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Scikit-learn library\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Visualizations\n",
    "from PIL import Image\n",
    "import imgaug as aug\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg # images\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "The initial step involves loading the dataset of chest X-ray images, each labelled with annotations indicating the presence of pneumonia or normal conditions. Following this, we standardise the size of all images to ensure uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and resizing images\n",
    "\n",
    "labels = ['PNEUMONIA', 'NORMAL']\n",
    "img_size = 150\n",
    "\n",
    "def get_training_data(data_dir):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            img_path = os.path.join(path, img)\n",
    "            img_arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img_arr is not None:\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "                data.append([resized_arr, class_num])\n",
    "            else:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "    return np.array(data, dtype=object)\n",
    "\n",
    "\n",
    "# loading data\n",
    "train_data_folder = 'E:/Chest_Xray/chest_xray/train'\n",
    "test_data_folder = 'E:/Chest_Xray/chest_xray/test'\n",
    "val_data_folder = 'E:/Chest_Xray/chest_xray/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then determine the quantity of images in the training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'E:/Chest_Xray/chest_xray/train\\\\PNEUMONIA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a1e9f23cb020>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Count the number of images in each dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_images_in_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PNEUMONIA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                 \u001b[0mcount_images_in_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NORMAL'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a1e9f23cb020>\u001b[0m in \u001b[0;36mcount_images_in_folder\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Function to count the number of image files in a folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcount_images_in_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Count the number of images in each dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'E:/Chest_Xray/chest_xray/train\\\\PNEUMONIA'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to count the number of image files in a folder\n",
    "def count_images_in_folder(folder):\n",
    "    return len([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "\n",
    "# Count the number of images in each dataset\n",
    "train_images = count_images_in_folder(os.path.join(train_data_folder, 'PNEUMONIA')) + \\\n",
    "                count_images_in_folder(os.path.join(train_data_folder, 'NORMAL'))\n",
    "\n",
    "test_images = count_images_in_folder(os.path.join(test_data_folder, 'PNEUMONIA')) + \\\n",
    "               count_images_in_folder(os.path.join(test_data_folder, 'NORMAL'))\n",
    "\n",
    "val_images = count_images_in_folder(os.path.join(val_data_folder, 'PNEUMONIA')) + \\\n",
    "              count_images_in_folder(os.path.join(val_data_folder, 'NORMAL'))\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Number of images in the training set: {train_images}\")\n",
    "print(f\"Number of images in the testing set: {test_images}\")\n",
    "print(f\"Number of images in the validation set: {val_images}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check and print the count of files in the different sub-folders corresponding to the specified labels for the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 4,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file(dir=None, labels=None):\n",
    "  for label in labels:\n",
    "    num_data= len(os.listdir(os.path.join(dir, label)))\n",
    "    print(f'Count of {label} : {num_data}')\n",
    "    \n",
    "labels= ['PNEUMONIA','NORMAL']\n",
    "print('Train Set: \\n' + '='*50)\n",
    "count_file(train_data_folder,labels)\n",
    "\n",
    "print('\\nValidation Set: \\n' + '='*50)\n",
    "count_file(val_data_folder,labels)\n",
    "\n",
    "print('\\nTest Set: \\n' + '='*50)\n",
    "count_file(test_data_folder,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "Then we validate for corrupt files: It is crucial to identify and eliminate corrupt images as they can lead to errors during the model training or introduce biases. \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 5,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corrupt files and remove them if any.\n",
    "\n",
    "def check_for_corrupt_files(directory):\n",
    "    corrupt_files = []\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            try:\n",
    "                with Image.open(filepath) as img:\n",
    "                    img.verify()  \n",
    "            except (IOError, SyntaxError) as e:\n",
    "                corrupt_files.append(filepath)\n",
    "\n",
    "    return corrupt_files\n",
    "\n",
    "directory = 'chest_xray'  \n",
    "corrupt_files = check_for_corrupt_files(directory)\n",
    "\n",
    "if corrupt_files:\n",
    "    print(\"Found corrupt or unreadable files:\")\n",
    "    for file in corrupt_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No corrupt files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Image Quality Assessment: Manually review a subset of images allowing for an evaluation of their quality in terms of focus, lighting, and clarity. Images of low quality may adversely affect the model's learning process."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 6,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the quality of the images\n",
    "def check_image_quality(directory, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Checks for images that are too dark or too bright, indicating potential quality issues.\n",
    "    :param directory: Directory containing the image dataset.\n",
    "    :param threshold: Fraction of pixels that are either black or white to consider the image as poor quality.\n",
    "    \"\"\"\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            try:\n",
    "                with Image.open(filepath) as img:\n",
    "                    img_array = np.array(img)\n",
    "                    # Check if the image is grayscale or colored\n",
    "                    if len(img_array.shape) == 2 or img_array.shape[2] == 1:  # Grayscale image\n",
    "                        n_white = np.sum(img_array >= 255)\n",
    "                        n_black = np.sum(img_array <= 0)\n",
    "                    else:  # Colored image\n",
    "                        n_white = np.sum(np.all(img_array >= [255, 255, 255], axis=2))\n",
    "                        n_black = np.sum(np.all(img_array <= [0, 0, 0], axis=2))\n",
    "\n",
    "                    # Check if the image has too many black or white pixels\n",
    "                    if (n_white + n_black) / img_array.size > threshold:\n",
    "                        print(f'Image {file} in {subdir} might be of poor quality')\n",
    "            except IOError:\n",
    "                # This can catch errors in opening the image file\n",
    "                print(f'Error opening {file} in {subdir}')\n",
    "\n",
    "check_image_quality('chest_xray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Detecting whether an image is blurred based on the variance of the Laplacian. This is a common technique to identify blurred images in image processing applications."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 7,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for blurred images\n",
    "\n",
    "def is_blurred(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return fm < 100  # threshold for blurriness; adjust as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need the images to have a consistent size for further analysis or model input. So we adjust the desired_size parameter according to our requirements."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 8,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing the images while maintaining their original aspect ratio\n",
    "def normalize_aspect_ratio(image_path, desired_size=224):\n",
    "    image = Image.open(image_path)\n",
    "    old_size = image.size\n",
    "\n",
    "    ratio = float(desired_size) / max(old_size)\n",
    "    new_size = tuple([int(x * ratio) for x in old_size])\n",
    "    image = image.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
    "    new_im.paste(image, ((desired_size - new_size[0]) // 2,\n",
    "                        (desired_size - new_size[1]) // 2))\n",
    "\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Enhancing fine details and improving the overall appearance of images."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 9,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhancing the images\n",
    "def enhance_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    return cv2.detailEnhance(image, sigma_s=10, sigma_r=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Printing one of the images"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 10,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the uploaded image, including the file name\n",
    "image_path = 'E:/Chest_Xray/chest_xray/train/NORMAL/IM-0115-0001.jpeg'\n",
    "\n",
    "# Define the image enhancement function\n",
    "def enhance_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    # Check if the image was correctly loaded\n",
    "    if image is not None:\n",
    "        enhanced_image = cv2.detailEnhance(image, sigma_s=10, sigma_r=0.15)\n",
    "        return enhanced_image\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No image found at {image_path}\")\n",
    "\n",
    "# Enhance the image\n",
    "enhanced = enhance_image(image_path)\n",
    "\n",
    "# Convert color from BGR to RGB (OpenCV uses BGR by default)\n",
    "enhanced_rgb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(enhanced_rgb)\n",
    "plt.axis('off')  # Turn off axis numbers\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Preparation & Pre-processing:\n",
    "\n",
    "Data augmentation techniques serve the purpose of artificially increasing the diversity of a dataset by applying various transformations to the existing data. In the context of image data, augmentation involves creating new images by making slight modifications to the original ones. The primary purposes of augmentation techniques are: increased diversity, improved robustness, reduced over-fitting, better generalisations and enhanced training efficiency.\n",
    "\n",
    "Setting 'shuffle=True' for the training generator will randomize the order of our training samples during each epoch. This is beneficial for preventing the model from memorizing the order of the training data and helps improve generalization."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 11,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator object for train set with shuffle enabled\n",
    "train_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                    rotation_range=20,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2,\n",
    "                                    shear_range=0.2,\n",
    "                                    zoom_range=0.2,\n",
    "                                    horizontal_flip=True\n",
    "                                     ).flow_from_directory(train_data_folder,\n",
    "                                                    target_size=(700,700),\n",
    "                                                    batch_size=32,shuffle=True)\n",
    "\n",
    "# create geenrator object for val set\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(val_data_folder, target_size=(700,700),\n",
    "                                                    batch_size = 32,shuffle=False)\n",
    "\n",
    "# # create geenrator object for test set\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(test_data_folder, target_size=(700,700),\n",
    "                                                    batch_size = 32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "execution_count": null,
========
   "execution_count": 12,
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for each set\n",
    "train_value_counts = Counter(train_generator.classes)\n",
    "val_value_counts = Counter(val_generator.classes)\n",
    "test_value_counts = Counter(test_generator.classes)\n",
    "\n",
    "# Display the value counts\n",
    "print(\"Train Set Value Counts:\")\n",
    "for class_name, count in train_value_counts.items():\n",
    "    print(f\"{class_name}: {count} images\")\n",
    "\n",
    "print(\"\\nValidation Set Value Counts:\")\n",
    "for class_name, count in val_value_counts.items():\n",
    "    print(f\"{class_name}: {count} images\")\n",
    "\n",
    "print(\"\\nTest Set Value Counts:\")\n",
    "for class_name, count in test_value_counts.items():\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the results generated are the same as the original dataset before separating the normal and pneumonia  chest x-rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "display_name": "Python 3",
========
   "display_name": "Python (munge-env)",
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
   "language": "python",
   "name": "munge-env"
>>>>>>> main
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.5"
=======
<<<<<<<< HEAD:Data Cleaning.ipynb
   "version": "3.8.5"
========
   "version": "3.10.13"
>>>>>>>> main:.ipynb_checkpoints/Data Cleaning-checkpoint.ipynb
>>>>>>> main
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
